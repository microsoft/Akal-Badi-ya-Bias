{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import optim, nn, utils, Tensor\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import wandb\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42, workers=True)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bias_classifier(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.bert = model\n",
    "        self.accuracy = BinaryAccuracy()\n",
    "        self.training_step_outputs = defaultdict(list)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        loss, logits = self(input_ids, attention_mask, labels)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.accuracy(labels,preds)\n",
    "        self.training_step_outputs[\"acc\"].append(acc)\n",
    "        self.training_step_outputs[\"loss\"].append(loss)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        train_loss_mean = torch.stack([x for x in self.training_step_outputs[\"loss\"]]).mean()\n",
    "        self.log('train_loss', train_loss_mean.item(), prog_bar=True)\n",
    "        train_acc_mean = torch.stack([x for x in self.training_step_outputs[\"acc\"]]).mean()\n",
    "        self.log('train_acc', train_acc_mean.item(), prog_bar=True)\n",
    "        self.training_step_outputs.clear()\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        loss, logits = self(input_ids, attention_mask, labels)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.accuracy(labels,preds)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc',acc, prog_bar=True)\n",
    "        \n",
    "        \n",
    "        return preds, labels\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        loss, logits = self(input_ids, attention_mask, labels)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.accuracy(labels,preds)\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc',acc, prog_bar=True)\n",
    "        \n",
    "        return preds, labels\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=1e-5)\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return outputs.loss, outputs.logits\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        \n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        loss, logits = self(input_ids, attention_mask)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        return preds\n",
    "\n",
    "bias = bias_classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_biased = np.load('PATH TO CORGI BIASED CORPUS',allow_pickle=True,fix_imports=True).item()\n",
    "all_data_nb = np.load('PATH TO CORGI NON BIASED CORPUS',allow_pickle=True,fix_imports=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_texts = list(all_data_biased['train']['ori_sentence']) + list(all_data_nb['train']['text'])\n",
    "val_input_texts = list(all_data_biased['valid']['ori_sentence']) + list(all_data_nb['valid']['text'])\n",
    "test_input_texts = list(all_data_biased['test']['ori_sentence']) + list(all_data_nb['test']['text'])\n",
    "\n",
    "train_labels = [1]*len(list(all_data_biased['train']['ori_sentence'])) + [0] *len(list(all_data_nb['train']['text']))\n",
    "val_labels = [1]*len(list(all_data_biased['valid']['ori_sentence'])) + [0] *len(list(all_data_nb['valid']['text']))\n",
    "test_labels = [1]*len(list(all_data_biased['test']['ori_sentence'])) + [0] *len(list(all_data_nb['test']['text']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = CustomDataset(train_input_texts, train_labels, tokenizer)\n",
    "val_dataset = CustomDataset(val_input_texts, val_labels, tokenizer)\n",
    "test_dataset = CustomDataset(test_input_texts, test_labels, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "early_stopping = EarlyStopping('val_loss', patience=5)\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", save_top_k = -1)\n",
    "trainer = pl.Trainer(max_epochs=10, accelerator=\"gpu\",deterministic=True,callbacks=[checkpoint_callback]))\n",
    "trainer.fit(model=bias, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "for files in os.listdir(\"/home/t-rishavhada/Desktop/sanmati_real_world_mining/corgi/checkpoints/\"):\n",
    "    if files.endswith(\".ckpt\"):\n",
    "        l = trainer.test(model=bias,dataloaders=test_dataloader,verbose=True, ckpt_path=\"/home/t-rishavhada/Desktop/sanmati_real_world_mining/corgi/checkpoints/\"+files)\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFERENCE\n",
    "to_pred = pd.read_csv(\"PATH TO INFERENCE FILE\")\n",
    "pred_input_texts = to_pred[\"comments\"].to_list()\n",
    "cleaned_list = [item if not isinstance(item, float) or not math.isnan(item) else \"\" for item in pred_input_texts]\n",
    "pred_labels = [99]*len(pred_input_texts)\n",
    "\n",
    "pred_dataset = CustomDataset(cleaned_list, pred_labels, tokenizer)\n",
    "\n",
    "pred_dataloader = DataLoader(pred_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "l = trainer.predict(model=bias,dataloaders=pred_dataloader, ckpt_path=\"PATH TO BEST MODEL\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [item.item() for tensor in l for item in tensor]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
